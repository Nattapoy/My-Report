{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import cv2\n",
    "import argparse\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial landmark \n",
    "เป็นการหาตำแหน่ง ของ ตา คิ้ว จมูก ปาก และโครงหน้า และพล็อตจุดในตามโครงสร้าง\n",
    "ขั้นแรก จะหาตำแหน่งของใบหน้า (Face Detection)และหาขอบของใบหน้า จากนั้นเราก็จะทำการพล็อตจุด ตามโครงสร้างที่ได้รับมา ลงบนใบหน้าในรูป\n",
    "การวางตำแหน่งของ [\"Facial landmark\"](https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup.jpg) จากตำอย่างเราใช้การพล็อตจุดแบบ 68 (x, y)-coordinates โดยแต่ล่ะจุดจะถูกกำหนดตำแหน่งเป็น x,y -coordinates\n",
    "ซึ่งเราสามารถที่จะ พล็อตจุดทั่วทั้งใบหน้า (shape ) หรือจะเลือกการพล็อตในบริเวณที่สนใจ โดยใช้ face_utils.FACIAL_LANDMARKS_IDXS[\" \"] เลือตำแหน่งที่สนใจ และสุดท้าย ให้สร้างตัวแปร มาเก็บจุด (x, y) ทั้งหมด ของตำแหน่งนั้น  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n",
      "Detection 0: Left: 138 Top: 188 Right: 361 Bottom: 411\n"
     ]
    }
   ],
   "source": [
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "img = cv2.imread(\"Monja-Radostics-8076_lowRes.jpg\",1)\n",
    "#the locasion for the eyebrows\n",
    "(blStart, blEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"] \n",
    "(brStart, brEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]\n",
    "#the locasion for the jawline\n",
    "(jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "\n",
    "\n",
    "\n",
    "dets = detector(img, 1)\n",
    "print(\"Number of faces detected: {}\".format(len(dets)))\n",
    "for k, d in enumerate(dets):\n",
    "    print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
    "        k, d.left(), d.top(), d.right(), d.bottom()))\n",
    "    # Get the landmarks/parts for the face in box d.\n",
    "    \n",
    "    shape = predictor(img, d)\n",
    "    shape = face_utils.shape_to_np(shape)  #convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "    left_eyebrow = shape[blStart:blEnd]\n",
    "    right_eyebrow = shape[brStart:brEnd]\n",
    "    jawline = shape[jStart:jEnd]\n",
    "    \n",
    "# Draw the face Detection on the screen.\n",
    "    x =  d.left()\n",
    "    y = d.top()\n",
    "    w = d.right() - d.left()\n",
    "    h = d.bottom() -d.top()\n",
    "    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "    \n",
    "#loop over the (x, y)-coordinates for the facial landmarks and draw them on the image\n",
    "#All 68 Point\n",
    "    for (x, y) in shape: \n",
    "        cv2.circle(img, (x, y ), 2, (0, 0, 255), -1)\n",
    "        \n",
    "    #plotting from x1,y1 to x17,17    \n",
    "    for (x, y) in jawline: \n",
    "        cv2.circle(img, (x, y ), 3, (255, 0, 0), -1)\n",
    "    # eyebrow plotting    \n",
    "    for (x, y) in left_eyebrow: \n",
    "        cv2.circle(img, (x, y ), 3, (0, 255, 0), -1)    \n",
    "    for (x, y) in right_eyebrow: \n",
    "        cv2.circle(img, (x, y ), 3, (0, 255, 0), -1) \n",
    "    \n",
    "\n",
    "    \n",
    "cv2.imshow(\"Image\",img) \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import cv2\n",
    "import argparse\n",
    "from imutils import face_utils\n",
    "\n",
    "###Playing Video from file\n",
    "cap=cv2.VideoCapture(\"3 Year Old Headbanging-a5BuwkqtIa0.webm\")\n",
    "\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "\n",
    "while True:\n",
    "    ret, frame= cap.read()\n",
    "    if ret:\n",
    "       \n",
    "        img= cv2.resize( frame, (640,480))\n",
    "        dets = detector(img, 1)\n",
    "        #print(\"Number of faces detected: {}\".format(len(dets)))\n",
    "        for k, d in enumerate(dets):\n",
    "           \n",
    "                # Get the landmarks/parts for the face in box d.\n",
    "            shape = predictor(img, d)\n",
    "            shape = face_utils.shape_to_np(shape)  #convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "            mouth = shape[mStart:mEnd]\n",
    "            x = d.left()\n",
    "            y = d.top()\n",
    "            w = d.right() - d.left()\n",
    "            h = d.bottom() -d.top()\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "\n",
    "            # Draw the face landmarks on the screen.\n",
    "            #loop over the (x, y)-coordinates for the facial landmarks and draw them on the image (Mouth)\n",
    "\n",
    "            for (x, y) in mouth:  \n",
    "                cv2.circle(img, (x, y ), 2, (0, 0, 255), -1)\n",
    "\n",
    "\n",
    "            # Display the resulting frame\n",
    "    cv2.imshow('frame',img)\n",
    "\n",
    "    ch=cv2.waitKey(1)       #ปรับความเร็ว\n",
    "    if ch & 0xFF== ord('q'):\n",
    "             break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye blink detection\n",
    "เป็นวิธีการตรวจสอบและการนับจำนวนการกระพริบ โดยใช้ Facial landmark เข้ามาช่วย โดยใช้หลัการง่ายๆคื่อ เมื่อกระพริบตา ระยะห่างของขอบตาในแนวตั้ง จะมีค่าเข้าใกล้ 0 แต่เพื่อลดการแปรในการวัดระยะที่อาจจะเกิดจากการหมุน เราจึงใช้ [Eye aspect ratio](https://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf) (EAR) ซึ่งใช้อัตราส่วน ระยะห่างระหว่างขอบตาในแนวนอน และระยะห่างหว่างขอบตาในแนวตั้ง มาใช่ในการตรวจสอบ \n",
    "เมื่อเราได้ function ที่จะใช้ในการหา EAR แล้ว ขั้นตอนต่อไปคือ การหาใบหน้าภายในภาพ Face Detection \n",
    "ขั้นตอนที่สาม เราจะระบุตำแหน่งของตาทั้งสองข้างโดย ใช่  facial landmark (x, y)-coordinates  ในกรณีนี้เราสนใจแค่ตำแหน่งของตา เราจึงทำการพล็อตจุด ณ แค่ตำแหน่งตา\n",
    "\n",
    "จากนั้นก็มาเริ่มนับ frame ที่มี EAR น้อยกว่า 0.3 และเมื่อใดก็ตามที่ EAR มากกว่า 0.3 แล้ว counter ของ frame มากกว่าหรือเท่ากับ 3 แต่ต้องน้อยกว่า 10 จะถือว่าเป็นการกระพริบตาหนึ่งครั้ง ถ้านอกหนือจากเงื่อนไขนี้ ไม่นับ เพราะถือว่า เป็นการหลับตา หรือการกระพริบการถี่ๆ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EYE_AR_THRESH = 0.25\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "COUNTER = 0\n",
    "TOTAL = 0\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    # compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# the locasion for the eyes\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "vs = FileVideoStream(\"S_8520928440646.mp4\").start() #import Viedo\n",
    "fileStream = True\n",
    "time.sleep(1)\n",
    "\n",
    "while True:\n",
    "    if fileStream and not vs.more():\n",
    "        break\n",
    "    \n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "\n",
    "    # detect faces in the grayscale frame\n",
    "    rects = detector(frame, 0)\n",
    "    \n",
    "    for k, d in enumerate(rects):\n",
    "\n",
    "            # Get the landmarks/parts for the face in box d.\n",
    "        shape = predictor(frame, d)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        leftEye = shape[lStart:lEnd]    \n",
    "        rightEye = shape[rStart:rEnd]     \n",
    "        \n",
    "        leftEAR = eye_aspect_ratio(leftEye)         #(x, y)-coordinates for the facial landmarks \n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    " \n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x =  d.left()\n",
    "        y = d.top()\n",
    "        w = d.right() - d.left()\n",
    "        h = d.bottom() -d.top()\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            \n",
    "        for (x, y) in leftEye: \n",
    "            cv2.circle(frame, (x, y ), 2, (0, 0, 255), -1)\n",
    "                \n",
    "        for (x, y) in  rightEye: \n",
    "            cv2.circle(frame, (x, y ), 2, (0, 0, 255), -1)\n",
    "            \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    " \n",
    "        # otherwise, the eye aspect ratio is not below the blink\n",
    "        # threshold\n",
    "        else:\n",
    "             # if the eyes were closed for a sufficient number of\n",
    "            # then increment the total number of blinks\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES and COUNTER < 10 :\n",
    "                TOTAL += 1\n",
    "\n",
    "                # reset the eye frame counter\n",
    "            COUNTER = 0\n",
    "        # Text output    \n",
    "        cv2.putText(frame, \"Blinks: {}\".format(TOTAL), (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Counter: {}\".format(COUNTER), (150, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "       \n",
    "     \n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    ch=cv2.waitKey(10)       #ปรับความเร็ว\n",
    "    if ch & 0xFF== ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouth Detection\n",
    "เป็นการตรวจสอบการอ้าปาก หุบปาก หรือการยิงฟันโดยใช้หลัการเดียวกับ Eye blink detection คือการใช้ aspect ratio ของจุดขอบล่างของ ริมฝีปากบนและล่าง มาตรวจสอบ โดยตั้งเงื่อนไขว่าถ้าหาก aspect ratio มีค่า น้อยกว่าหรือเท่ากับ 0.05 จะถือว่า เป็นการหุปปาก แต่เมื่อ aspect ratio มากกว่า 0.05 แต่น้อยกว่าหรือเท่ากับ 0.35 แสดงว่าคนในวีดีโอ กำลังยิ้มหรือยิงฟัน และเมื่อใดที่ ที่ aspect ratio มากกว่า 0.35 จะถือเป็นการอ้าปาก และจะทำการวัดระยะความกว้างของการอ้าปากออกมา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MOUTH_IS_CLOSE = 0.05\n",
    "SMILE = 0.35\n",
    "\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# the locasion for the Mouth\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "\n",
    "vs = FileVideoStream(\"video_Yr2018M7D19h23m8s2.mp4\").start() #import Viedo\n",
    "fileStream = True\n",
    "time.sleep(1)\n",
    "\n",
    "while True:\n",
    "    if fileStream and not vs.more():\n",
    "        break\n",
    "    \n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=680)\n",
    "\n",
    "    # detect faces in the grayscale frame\n",
    "    rects = detector(frame, 0)\n",
    "    \n",
    "    for k, d in enumerate(rects):\n",
    "\n",
    "            # Get the landmarks/parts for the face in box d.\n",
    "        shape = predictor(frame, d)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        lMouth = shape[mStart:mEnd]   \n",
    "        \n",
    "        A = dist.euclidean(lMouth[13], lMouth[19])\n",
    "        B = dist.euclidean(lMouth[14], lMouth[18])\n",
    "        C = dist.euclidean(lMouth[15], lMouth[17])\n",
    " \n",
    "        D = dist.euclidean(lMouth[12], lMouth[16])\n",
    "\n",
    "        # compute the  aspect ratio\n",
    "        mar = (A + B+C) / (3.0 * D)     \n",
    "        Distance = (A + B+C) / 3.0\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        x =  d.left()\n",
    "        y = d.top()\n",
    "        w = d.right() - d.left()\n",
    "        h = d.bottom() -d.top()\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            \n",
    "        for (x, y) in lMouth: \n",
    "            cv2.circle(frame, (x, y ), 1, (0, 0, 255), -1)\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "        if mar <= MOUTH_IS_CLOSE:\n",
    "            cv2.putText(frame, \"Mouth is close \", (d.left() - 10, d.top() - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "    # otherwise, the MOUTH aspect ratio is \n",
    "        \n",
    "        else:\n",
    "            if mar <= SMILE:\n",
    "                cv2.putText(frame, \"show the teeth \", (d.left() - 10, d.top() - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2) \n",
    "            \n",
    "            else:\n",
    "                cv2.putText(frame, \"Mouth is open: {:.2f}\".format( Distance), (d.left() - 10, d.top() - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "\n",
    "#         cv2.putText(frame, \"MAR: {:.2f}\".format(mar), (300, 30),\n",
    "#             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "     \n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    ch=cv2.waitKey(10)       #ปรับความเร็ว\n",
    "    if ch & 0xFF== ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "#plt.show() \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Face Aligmen\n",
    "ใช้ในการจัดของประกอบของหน้า  เมื่อหน้ามีการเอียงหรือการหมุน เพื่อให้การวัดขนาดเป็นไปได้ง่าย และแม่นยำมากขึ้น \n",
    "\n",
    "โดยเริ่มจากการ Detect ตาทั้งสองข้าง และหาจุดกึ่งกลางของตาแต่ล่ะข้าง และมุมที่กึ่งกลางของตากระทำต่อกัน ขั้นตอนต่อไปคือระบุอัตราส่วนของหน้า โดยให้ อัตราส่วนของระยะห่างระหว่างตา ใน current image เท่ากับ  อัตราส่วนของระยะห่างระหว่างตา ใน desires image  \n",
    "จากจุดกึ่งกลางของตาทั้งสองข้างเราก็จะสามารถหาจุดที่อยู่กึ่งกลางระหว่างตาทั้งสองข้างได้  และเราจะใช้จุด จุดนี้ในการคำนวณหา Rotation Matrix เมื่อเราได้ Matrix มาแล้วก็นำไป transformation **cv2.warpAffine()**\n",
    "\n",
    "โดยวิธีการข้างต้นนี้เราสามารถนำเข้ามาจาก **libaries imutils**  จากนั้นก็เลือใช้คำสั่ง **FaceAligner()** จากตัวอย่างเราได้สร้างตัวแปร fa เพื่อทำการเก็บคำสั่งนี้ และเพื่อง่ายต่อการ align หน้า ต้องทำให้ภาพเป็น binary Image เสียก่อน แล้วจะทำการสร้าง loop เพื่อหมุนตาให้อยู่ในแนวเดียวกับ horizontal line จากคำสั้ง **fa.align(img, gray,i)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image Face Aligment\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "from imutils.face_utils import shape_to_np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "img = cv2.imread(\"Monja-Radostics-8076_lowRes.jpg\",1)\n",
    "img = imutils.resize(img, width=800)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "dets = detector(gray, 1)\n",
    "\n",
    "\n",
    "for k, d in enumerate(dets):\n",
    "\n",
    "    shape = predictor(img, d)\n",
    "    shape = shape_to_np(shape)  #convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "    for (x, y) in shape: \n",
    "        cv2.circle(img, (x, y ), 2, (0, 0, 255), -1)\n",
    "    for i in dets:\n",
    "        (x, y, w, h) = rect_to_bb(i)\n",
    "        faceAligned = fa.align(img, gray,i)\n",
    "        \n",
    "        \n",
    "cv2.imshow(\"Aligned\", faceAligned)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Video Face Aligment\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "vs = FileVideoStream(\"video_Yr2018M7D19h23m8s2.mp4\").start() #import Viedo\n",
    "fileStream = True\n",
    "\n",
    "#cv2.waitKey(10) \n",
    "while True:\n",
    "    if fileStream and not vs.running():\n",
    "        break\n",
    "    \n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=680)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    dets = detector(gray, 1)\n",
    "\n",
    "    for k, d in enumerate(dets):\n",
    "            \n",
    "        shape = predictor(frame, d)\n",
    "        shape = shape_to_np(shape)  #convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "            \n",
    "        for (x, y) in shape: \n",
    "            cv2.circle(frame, (x, y ), 2, (0, 0, 255), -1)\n",
    "        \n",
    "        for i in dets:\n",
    "            (x, y, w, h) = rect_to_bb(i)\n",
    "            Aligned = fa.align(frame, gray,i)\n",
    "        \n",
    "\n",
    "            # Display the resulting frame\n",
    "    cv2.imshow(\"Aligned\", Aligned)\n",
    "\n",
    "    ch=cv2.waitKey(1)       \n",
    "    if ch == ord('q'):\n",
    "        break\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
